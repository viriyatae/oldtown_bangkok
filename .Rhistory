options(scipen = 7, digits = 3)
View(workplace)
# Only select the columns we want to analyse to reduce confusion!
workplace_1 <- workplace %>%
dplyr::select(9:14)
View(workplace_1)
# 2.1 Using the elbow method to indicate the most suitable K ----
fviz_nbclust(workplace_1, kmeans, method = "wss", k.max = 20)
# 2.2 Using the silhouette method ----
fviz_nbclust(workplace_1, kmeans, method = "silhouette", k.max = 20)
# 2.1 Using the elbow method to indicate the most suitable K ----
fviz_nbclust(workplace_1, kmeans, method = "wss", k.max = 30)
# 3. CLUSTER ANALYSIS ----
## Determine the most appropriate number of k
k <- 4
# 3. CLUSTER ANALYSIS ----
## Determine the most appropriate number of k
k <- 4
## Because cluster analysis is random, set seed to ensure reproducibility
set.seed(2021)
## Because cluster analysis is random, set seed to ensure reproducibility
set.seed(2021)
## Perform a cluster analysis with the k number of clusters
## iter.max is the number of iterations in each round
## nstart is the number of times we do cluster analysis to select the best one
km_output_new <- kmeans(workplace_1, centers = k, iter.max = 50, nstart = 25)
View(km_output_new)
km_output_new[["cluster"]]
km_output_new[["centers"]]
km_output_new$centers
## Observe the cluster centers #
(km_centers <- as.data.frame(km_output_new$centers))
## Observe the cluster centers #
km_centers <- as.data.frame(km_output_new$centers)
## Observe the cluster centers #
km_centers <- as.data.frame(km_output_new$centers)
## Observe the cluster centers #
(km_centers <- as.data.frame(km_output_new$centers))
View(km_centers)
# 4. VISUALISE ----
## Visualise the cluster centers #
## First, have to adjust the data structure to be suitable for visualisation
km_centers_new <- km_centers %>%
mutate(cluster = as.factor(1:k)) %>%
pivot_longer(cols = 1:length(km_centers), values_to = "importance", names_to = "attribute")
View(km_centers_new)
## Plot
ggplot(km_centers_new, aes(x = cluster, y = importance, fill = attribute)) +
geom_col(position = "dodge") +
theme_light() +
scale_fill_viridis_d(option = "cividis")
km_output_new$cluster
View(workplace)
km_output_new$cluster
workplace_new <- workplace %>%
bind_cols(Cluster = as.factor(km_output_new$cluster)) # Bind the new column "cluster" to the original dataset
View(workplace_new)
## Cross-tab analysis between Cluster and Gender
table(workplace_new$Gender, workplace_new$Cluster) # Checking the frequencies by Gender and Cluster
## Cross-tab analysis between Cluster and Gender
table(workplace_new$Cluster) # Checking the frequencies by Gender and Cluster
## Cross-tab analysis between Cluster and Gender
table(workplace_new$Gender, workplace_new$Cluster) # Checking the frequencies by Gender and Cluster
chisq.test(workplace_new$Gender, workplace_new$Cluster) # Perform a chi-square test to see the relationship (check p value)
prop.table(table(workplace_new$Gender, workplace_new$Cluster), margin = 1) # Transform from frequency to percentage BY ROW
prop.table(table(workplace_new$Gender, workplace_new$Cluster), margin = 2) # Transform from frequency to percentage BY COLUMN
## Cross-tab analysis between Cluster and Generation (Gen)
table(workplace_new$Gen, workplace_new$Cluster) # Checking the frequencies by Gender and Cluster
chisq.test(workplace_new$Gen, workplace_new$Cluster) # Perform a chi-square test to see the relationship (check p value)
chisq.test(workplace_new$Gen, workplace_new$Cluster) # Perform a chi-square test to see the relationship (check p value)
prop.table(table(workplace_new$Gen, workplace_new$Cluster), margin = 1) # Transform from frequency to percentage BY ROW
prop.table(table(workplace_new$Gen, workplace_new$Cluster), margin = 2) # Transform from frequency to percentage BY COLUMN
prop.table(table(workplace_new$Gen, workplace_new$Cluster), margin = 1) # Transform from frequency to percentage BY ROW
## Let's visualise a bar chart with the data workplace_new
## Hint: mapping = aes(x = Cluster, ___ = ___) [Hint2: we will separate bars of Gen using different colors]
## Add geom_bar() or geom_bar(position = "fill") after ggplot(), what's the difference?
ggplot(workplace_new, mapping = aes(x = Cluster, fill = Gen)) +
geom_bar()
## Let's visualise a bar chart with the data workplace_new
## Hint: mapping = aes(x = Cluster, ___ = ___) [Hint2: we will separate bars of Gen using different colors]
## Add geom_bar() or geom_bar(position = "fill") after ggplot(), what's the difference?
ggplot(workplace_new, mapping = aes(x = Cluster, fill = Gen)) +
geom_bar(position = "fill")
## Let's visualise a bar chart with the data workplace_new
## Hint: mapping = aes(x = Cluster, ___ = ___) [Hint2: we will separate bars of Gen using different colors]
## Add geom_bar() or geom_bar(position = "fill") after ggplot(), what's the difference?
ggplot(workplace_new, mapping = aes(x = Cluster, fill = Gen)) +
geom_bar(position = "fill") +
ylab("percentage")
## Let's visualise a bar chart with the data workplace_new
## Hint: mapping = aes(x = Cluster, ___ = ___) [Hint2: we will separate bars of Gen using different colors]
## Add geom_bar() or geom_bar(position = "fill") after ggplot(), what's the difference?
ggplot(workplace_new, mapping = aes(x = Cluster, fill = Gen)) +
geom_bar(position = "dodge")
tiktok <- read.csv("https://www.dropbox.com/s/n77wf06hf1fexad/TikTok_Questionnaire_Responses_TEAM5.csv?dl=1")
View(tiktok)
ikea <- read.csv("https://www.dropbox.com/s/67kpy224yi93jdw/Popcorn_%20IKEA%20Survey%20%28Responses%29%20-%20Copy%20of%20Form%20Responses%201%20%283%29.csv?dl=1")
View(ikea)
ikea <- read.csv("https://www.dropbox.com/s/67kpy224yi93jdw/Popcorn_%20IKEA%20Survey%20%28Responses%29%20-%20Copy%20of%20Form%20Responses%201%20%283%29.csv?dl=1")
ikea_1 <- ikea %>%
dplyr::select(13:17)
View(ikea_1)
ikea_1 <- ikea %>%
dplyr::select(c(13:17,19:21))
ikea_1 <- ikea %>%
dplyr::select(c(13,14,15,17,16,21,20,19))
# 2.1 Using the elbow method to indicate the most suitable K ----
fviz_nbclust(ikea_1, kmeans, method = "wss", k.max = 30)
# 2.2 Using the silhouette method ----
fviz_nbclust(ikea_1, kmeans, method = "silhouette", k.max = 20)
# 3. CLUSTER ANALYSIS ----
## Determine the most appropriate number of k
k <- 3
## Because cluster analysis is random, set seed to ensure reproducibility
set.seed(2021)
## Perform a cluster analysis with the k number of clusters
## iter.max is the number of iterations in each round
## nstart is the number of times we do cluster analysis to select the best one
km_output_new <- kmeans(ikea, centers = k, iter.max = 50, nstart = 25)
## Observe the cluster centers #
(km_centers <- as.data.frame(km_output_new$centers))
## Perform a cluster analysis with the k number of clusters
## iter.max is the number of iterations in each round
## nstart is the number of times we do cluster analysis to select the best one
km_output_new <- kmeans(ikea_1, centers = k, iter.max = 50, nstart = 25)
## Observe the cluster centers #
(km_centers <- as.data.frame(km_output_new$centers))
# 4. VISUALISE ----
## Visualise the cluster centers #
## First, have to adjust the data structure to be suitable for visualisation
km_centers_new <- km_centers %>%
mutate(cluster = as.factor(1:k)) %>%
pivot_longer(cols = 1:length(km_centers), values_to = "importance", names_to = "attribute")
## Plot
ggplot(km_centers_new, aes(x = cluster, y = importance, fill = attribute)) +
geom_col(position = "dodge") +
theme_light() +
scale_fill_viridis_d(option = "cividis")
View(km_centers)
View(km_centers_new)
# 4. VISUALISE ----
## Visualise the cluster centers #
## First, have to adjust the data structure to be suitable for visualisation
km_centers_new2 <- km_centers %>%
mutate(cluster = as.factor(1:k))
View(km_centers_new2)
length(km_centers)
View(km_centers_new)
# 4. VISUALISE ----
## Visualise the cluster centers #
## First, have to adjust the data structure to be suitable for visualisation
km_centers_new2 <- km_centers %>%
mutate(cluster = as.factor(1:k)) %>%
pivot_longer(cols = 1:7, values_to = "importance", names_to = "attribute")
View(km_centers_new2)
# 4. VISUALISE ----
## Visualise the cluster centers #
## First, have to adjust the data structure to be suitable for visualisation
km_centers_new2 <- km_centers %>%
mutate(cluster = as.factor(1:k)) %>%
pivot_longer(cols = 1:6, values_to = "importance", names_to = "attribute")
# 3. CLUSTER ANALYSIS ----
## Determine the most appropriate number of k
k <- 4
## Because cluster analysis is random, set seed to ensure reproducibility
set.seed(2021)
## Perform a cluster analysis with the k number of clusters
## iter.max is the number of iterations in each round
## nstart is the number of times we do cluster analysis to select the best one
km_output_new <- kmeans(workplace_1, centers = k, iter.max = 50, nstart = 25)
## Observe the cluster centers #
(km_centers <- as.data.frame(km_output_new$centers))
# 4. VISUALISE ----
## Visualise the cluster centers #
## First, have to adjust the data structure to be suitable for visualisation
km_centers_new <- km_centers %>%
mutate(cluster = as.factor(1:k)) %>%
pivot_longer(cols = 1:length(km_centers), values_to = "importance", names_to = "attribute")
## Plot
ggplot(km_centers_new, aes(x = cluster, y = importance, fill = attribute)) +
geom_col(position = "dodge") +
theme_light() +
scale_fill_viridis_d(option = "cividis")
spotify <- read.csv("https://www.dropbox.com/s/0cizt02x0kvvnmt/Spotify%20%20%28Responses%29%20-%20Copy%20of%20Form%20Responses%201.csv?dl=1")
pacman::p_load(pacman, dplyr, ggplot2, ggthemes, rio, readr, tidyr, broom, stringr,
corrplot, psych, gridExtra, tibble, caret, moderndive, purrr)
options(scipen = 7, digits = 3) # adjust to show scientific number, try 20 and check summary again
#Import the data
university_student <- read.csv("https://www.dropbox.com/s/varlgrwsm7i4m8q/university_student.csv?dl=1")
#Plot Satisfaction boxplot by University to see the different levels of satisfaction by uni
ggplot(university_student, aes(x=University,y=satisfaction)) + # in ggplot() input the data and the mapping
geom_boxplot() + # produce a boxplot of the SatSum of each university
theme_light() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # Adjust the axis text using the theme() function
View(university_student)
# 1. PLOT RELATIONSHIPS ----
#Plot relationship between instructor and satisfaction
ggplot(data = university_student, aes(x = instructor, y = satisfaction)) +
geom_jitter(alpha = 0.3) + # product points of all respondents from the two specified axes (InQSum nad SatSum)
geom_smooth(method = "lm") + # geom_smooth() produce a linear model (linear regression) between X and Y
theme_light()
#Plot relationship between administration and satisfaction
ggplot(data = university_student, aes(x = administration, y = satisfaction)) +
geom_jitter(alpha = 0.3) +
geom_smooth(method = "lm") +
theme_light()
#Plot relationship between social and satisfaction
ggplot(data = university_student, aes(x = social, y = satisfaction)) +
geom_jitter(alpha = 0.3) +
geom_smooth(method = "lm") +
theme_light()
#Plot relationship between physical and satisfaction
ggplot(data = university_student, aes(x = physical, y = satisfaction)) +
geom_jitter(alpha = 0.3) +
geom_smooth(method = "lm") +
theme_light()
#Plot relationship between curriculum and satisfaction
ggplot(data = university_student, aes(x = curriculum, y = satisfaction)) +
geom_jitter(alpha = 0.3) +
geom_smooth(method = "lm") +
theme_light()
# 2. CREATE A MULTIPLE REGRESSION MODEL ----
# Create a multiple regression model
model_student_sat <- lm(satisfaction ~ instructor + administration + physical +
social + curriculum, data = university_student)
#See the summary of the model, check the estimates and p-value (Pr(>|t|))
summary(model_student_sat)
# Bonus 1 Parallel slopes ----
## Plot a parallel slopes between InQSum and SatSum by type of university
ggplot(university_student, aes(x = instructor, y = satisfaction, color = Utype)) +
geom_jitter(alpha = 0.3) +
#geom_smooth(method = "lm") +
geom_parallel_slopes(se = FALSE) +
scale_color_viridis_d(option = "mako")
pacman::p_load(pacman, dplyr, ggplot2, ggthemes, rio, readr, tidyr, broom, stringr,
corrplot, psych, gridExtra, tibble, caret, moderndive, purrr, dotwhisker)
#Import the data
university_student <- read.csv("https://www.dropbox.com/s/varlgrwsm7i4m8q/university_student.csv?dl=1")
dwplot(model_student_sat)
dwplot(model_student_sat, ci = 0.95)
dwplot(model_student_sat, ci = 0.95, show_intercept = TRUE)
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE, vline = 0)
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE, vline = TRUE)
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE, vline = list(xintercept = 0))
geom_vline(xintercept = 0)
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE) +
geom_vline(xintercept = 0)
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE) +
geom_vline(xintercept = 0) +
scale_color_brewer(paletter = "Deark2")
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE) +
geom_vline(xintercept = 0) +
scale_color_brewer(paletter = "Dark")
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE) +
geom_vline(xintercept = 0) +
scale_color_brewer(paletter = "Dark2")
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE) +
geom_vline(xintercept = 0) +
scale_color_brewer(palette = "Dark2")
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE) +
geom_vline(xintercept = 0) +
theme_light()
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE) +
theme_light()
dwplot(model_student_sat, ci = 0.95, show_intercept = FALSE) +
theme_fivethirtyeight()
dwplot(model_student_sat, ci = 0.95, model_name = "Coefficients of Factors affecting University Satisfaction") +
theme_fivethirtyeight()
View(university_student)
dwplot(model_student_sat, ci = 0.95, model_name = University) +
theme_fivethirtyeight()
dwplot(model_student_sat, ci = 0.95, model_name = "University") +
theme_fivethirtyeight()
View(model_student_sat)
dwplot(model_student_sat, ci = 0.95, model_name = instructor) +
theme_fivethirtyeight()
dwplot(model_student_sat, ci = 0.95)
#See the summary of the model, check the estimates and p-value (Pr(>|t|))
summary(model_student_sat)
# 1. read the data ----
iphonex <- read.csv("https://www.dropbox.com/s/5vgzu02i4sgiecx/iphonex_purchase.csv?dl=1") %>%
mutate(likely_purchase = factor(likely_purchase,levels = c(0,1), labels = c("not likely","likely"))) %>%
dplyr::select(-id)
# 2. explore the data ----
## How many people likely / not likely purchase iPhone X?
table(iphonex$likely_purchase)
## Check the average of all variables
colMeans(iphonex[2:7])
# 3. visualise relationship between each independent variable and purchase likelihood ----
ggplot(iphonex, aes(x = awareness, y = likely_purchase)) +
geom_point(alpha = 0.5)
ggplot(iphonex, aes(x = identification, y = likely_purchase)) +
geom_point(alpha = 0.5)
ggplot(iphonex, aes(x = equity, y = likely_purchase)) +
geom_point(alpha = 0.5)
ggplot(iphonex, aes(x = features, y = likely_purchase)) +
geom_point(alpha = 0.5)
ggplot(iphonex, aes(x = value, y = likely_purchase)) +
geom_point(alpha = 0.5)
ggplot(iphonex, aes(x = social, y = likely_purchase)) +
geom_point(alpha = 0.5)
# 4. create a logistic regression model with glm() ----
iphonex_purchase_model <- glm(formula = likely_purchase ~ ., data = iphonex, family = "binomial")
## check the model
summary(iphonex_purchase_model)
## zoom in to the coefficient (log-odds)
tidy(iphonex_purchase_model)
## exp() exponentiate the log-odd to display the odds
tidy(iphonex_purchase_model)[1] %>% bind_cols(exp(tidy(iphonex_purchase_model)[2])) %>%
set_colnames(c("terms","odds"))
library(dplyr)
library(ggplot2)
library(broom)
library(magrittr)
## exp() exponentiate the log-odd to display the odds
tidy(iphonex_purchase_model)[1] %>% bind_cols(exp(tidy(iphonex_purchase_model)[2])) %>%
set_colnames(c("terms","odds"))
## How do you interpret?
tidy(iphonex_purchase_model)[1] %>% bind_cols((exp(tidy(iphonex_purchase_model)[2]) - 1) * 100) %>%
set_colnames(c("terms","percentage increase of odds"))
## How do you interpret?
tidy(iphonex_purchase_model)[1] %>% bind_cols((exp(tidy(iphonex_purchase_model)[2]) - 1) * 100) %>%
set_colnames(c("terms","percentage_increase_of_odds"))
dwplot(iphonex_purchase_model)
dwplot(iphonex_purchase_model, ci = 0.95) + geom_vline(xintercept = 0)
## check the model
summary(iphonex_purchase_model)
## Visualise the logistic regression results using dwplot()
dwplot(iphonex_purchase_model, ci = 0.95) + geom_vline(xintercept = 0) + xlab("Standardised Coefficients")
# Visualise the results using the Dot-and-Whisker plots dwplot()
dwplot(model_student_sat, ci = 0.95) + xlab("Standardised Coefficients")
pacman::p_load(dplyr, ggplot2, ggthemes, rio, stringr, tidyr, broom, viridisLite, ggrepel)
bangkok_raw <- import("https://www.dropbox.com/s/07t8mlxpyhp9cku/Resident_finish.xlsx?dl=1")
View(bangkok_raw)
# 1. Transform the data ----
bangkok <- bangkok_raw %>%
# mutate gender using ifelse and factor
mutate(gender = ifelse(a2 == 1,"male","female"),
gender = factor(gender, levels = c("male","female"))) %>%
# mutate age using factor and the argument "labels"
mutate(age = factor(a3, levels = 1:7, labels = c("<18","18-24","25-34","35-44","45-54","55-64",">64"))) %>%
# mutate income using case_when and factor
mutate(income = case_when(a6 == 1 | a6 == 2 ~ "low",
a6 == 3 | a6 == 4 ~ "middle",
a6 == 5 | a6 == 6 | a6 > 6 ~ "high"),
income = factor(income, levels = c("low","middle","high"))) %>%
# mutate and compute the averages of all constructs
mutate(activities = (c1+c2+c3+c4)/4,
economy = (c5+c6+c7+c8)/4,
nature = (c9+c10+c11+c12)/4,
socialisation = (c13+c14+c15+c16)/4,
transport = (c17+c18+c19+c20)/4,
satisfaction = (d1+d2+d3+d4)/4,
identification = (d5+d6+d7+d8)/4,
commitment = (d9+d10+d11+d12)/4,
ambassadorship = round((e1+e2+e3)/3,2),
citizenship = round((e4+e5+e6)/3,2))
View(bangkok)
## Check the means of all attributes (performance)
colMeans(bangkok[,60:64])
## Fit a lm model between the attributes and the desired outcome to imply importance
bangkok_model <- lm(ambassadorship ~ activities + economy + nature + socialisation + transport,
data = bangkok)
## The "implied importance" method use the estimates (coefficients) as the importance of attributes
(bangkok_coef <- tidy(bangkok_model))
# 2. Importance-Performance Analysis ----
## Create a tibble (or a dataframe) of the attribute, performance and importance
bangkok_ipa <- tibble(attribute = c("activities","economy","nature","socialisation","transport"),
performance = round(c(mean(bangkok$activities), # Performance is calcuated from the mean
mean(bangkok$economy),
mean(bangkok$nature),
mean(bangkok$socialisation),
mean(bangkok$transport)),2),
importance = abs(round(c(bangkok_coef$estimate[2], # Importance is derived from the estimates of the lm model
bangkok_coef$estimate[3],
bangkok_coef$estimate[4],
bangkok_coef$estimate[5],
bangkok_coef$estimate[6]),2)))
## Plot IPA
ggplot(data = bangkok_ipa,
mapping = aes(x = performance, y = importance, color = attribute)) + # Set data and mapping
geom_point() + # Plot points
ggrepel::geom_text_repel(aes(label = attribute), color = "black") + # Add text using geom_text_repel() from ggrepel package, texts won't overlap!
geom_vline(xintercept = mean(bangkok_ipa$performance), size = 0.1, color = "blue") + # Add a vertical line from the mean of performance
geom_hline(yintercept = mean(bangkok_ipa$importance), size = 0.1, color = "blue") + # Add a horizontal line from the mean of importance
theme_light() + # Set your theme
theme(legend.position = "none") + # Remove legend
xlab("Performance") + ylab("Importance") # Set axis titles
View(bangkok_ipa)
## Then, we use the p_load() function to install/load other required packages
pacman::p_load(rio,dplyr,ggplot2,leaflet,leaflet.extras,sp,htmlwidgets)
# 2. Import the polygon file ----
## The import() function of "rio" package helps import most file types
oldtown_spdf <- import("https://www.dropbox.com/s/cr7gz6v83nnym0w/oldtown_spdf.RData?dl=1")
## Let's visualise the polygon using the plot() function!
plot(oldtown_spdf)
# 3. Import the oldtown photos file ----
## Please explore the oldtown_raw dataframe using the view icon in the "envionment" panel
oldtown_raw <- read.csv("https://www.dropbox.com/s/y4y9kkiciw01oyc/oldtown.csv?dl=1")
## Let's assume that we performed a topic modelling algorithm (LDA), here's the final file
oldtown_lda <- read.csv("https://www.dropbox.com/s/e6lsz6a19364jdz/oldtown_lda.csv?dl=1")
View(oldtown_raw)
View(oldtown_spdf)
oldtown_spdf@data[["SUBDISTR_1"]]
View(oldtown_lda)
View(oldtown_lda)
setwd("~/")
setwd("~/Documents/GitHub/oldtown_bangkok")
## Then, we use the p_load() function to install/load other required packages
pacman::p_load(rio,dplyr,ggplot2,leaflet,leaflet.extras,sp,htmlwidgets)
# 2. Import the polygon file ----
## The import() function of "rio" package helps import most file types
oldtown_spdf <- import("oldtown_spdf.RData")
## Let's visualise the polygon using the plot() function!
plot(oldtown_spdf)
# 3. Import the oldtown photos file ----
## Please explore the oldtown_raw dataframe using the view icon in the "envionment" panel
oldtown_raw <- read.csv("oldtown.csv")
## Let's assume that we performed a topic modelling algorithm (LDA), here's the final file
oldtown_lda <- read.csv("oldtown_lda.csv")
# 4. Creating the first map ----
## After we have the dataframe, we are going to create a map using leaflet() function
oldtown_map <- oldtown_lda %>% # This "%>%" is the piping function that can chain many functions together, read it as "then"
# We then create a map using a leaflet() function, we also specify some other configurations, you can try removing and only have leaflet()
leaflet(option = leafletOptions(dragging = TRUE, minZoom = 4.7, maxZoom = 18)) %>%
# There is nothing on the map yet so we have to add an open-sourced map called "ESRI"
addProviderTiles("Esri.WorldStreetMap", group = "Esri") %>%
# We also add another map on top. These two maps (laying on top of one another) can be toggled later
addProviderTiles("CartoDB.Positron", group = "CartoDB") %>%
# We set the view and zoom to the oldtown Bangkok, please try adjusting the number
setView(lng = 100.503186,
lat = 13.746717,
zoom = 14)
# You can visualise the map by running the leaflet object that we just created
oldtown_map
# 5. Visualise the locations of photos ----
oldtown_map %>%
addCircleMarkers(lat = ~lat, lng = ~lng,
radius = 0.1, opacity = 0.1, color = "black")
# 6. Visualise the locations of photos by the region of residence of users ----
## Dot plot by region
oldtown_map %>%
# Let's add oldtown polygons on the map first
addPolygons(data = oldtown_spdf,
weight = 1, stroke = TRUE, color = "white", opacity = 0.8,
fillColor = "blue", fillOpacity = 0.1) %>%
# Add circle markers of photos taken by users from "thailand"
addCircleMarkers(data = oldtown_lda[oldtown_lda$region == "thailand",],
lng = ~lng,
lat = ~lat,
radius = 2, opacity = 0.3, color = "grey",
group = "Thailand") %>%
# Add circle markers of photos taken by users from "asia"
addCircleMarkers(data = oldtown_lda[oldtown_lda$region == "asia",],
lng = ~lng,
lat = ~lat,
radius = 2, opacity = 0.3, color = "red",
group = "Asia") %>%
# Add circle markers of photos taken by users from "europe"
addCircleMarkers(data = oldtown_lda[oldtown_lda$region == "europe",],
lng = ~lng,
lat = ~lat,
radius = 2, opacity = 0.3, color = "blue",
group = "Europe") %>%
# Add circle markers of photos taken by users from "namerica"
addCircleMarkers(data = oldtown_lda[oldtown_lda$region == "namerica",],
lng = ~lng,
lat = ~lat,
radius = 2, opacity = 0.3, color = "green",
group = "North America") %>%
# Add a control panel to toggle between the four regions
addLayersControl(baseGroups = c("Thailand","Asia","Europe","North America"),
position = "topright") %>%
addFullscreenControl()
# 7. Visualise the locations of photos by region and topic
## Before visualising, we need to create a list of region-topic pairing
layers <- list()
region_topic_grid <- expand.grid(topic_name = colnames(oldtown_lda)[11:24], region = c("thailand","asia","europe","namerica")) %>%
mutate(region_topic = paste(region,topic_name,sep = "_"))
for (i in 1:nrow(region_topic_grid)) {
layers[[i]] <- oldtown_lda %>%
filter(topic_name == as.character(region_topic_grid[i,1]),
region == as.character(region_topic_grid[i,2]))
}
## Draw a map
oldtown_dots <- oldtown_map %>%
## Let's add oldtown polygons on the map first
addPolygons(data = oldtown_spdf,
weight = 1, stroke = TRUE, color = "white", opacity = 0.8,
fillColor = "blue", fillOpacity = 0.1)
## Run a for loop ploting the layer of dots (users from Thailand). The reason we run separately is to change colors
for (i in 1:14) {
oldtown_dots <- oldtown_dots %>% addCircleMarkers(data = layers[[i]], group = as.character(region_topic_grid[i,3]), lng = ~lng, lat = ~lat, radius = 4, stroke = TRUE, weight = 1, opacity = 0.3, color = "black", fill = TRUE, fillColor = "grey", fillOpacity = 0.3)
}
for (i in 15:28) {
oldtown_dots <- oldtown_dots %>% addCircleMarkers(data = layers[[i]], group = as.character(region_topic_grid[i,3]), lng = ~lng, lat = ~lat, radius = 4, stroke = TRUE, weight = 1, opacity = 0.3, color = "black", fill = TRUE, fillColor = "red", fillOpacity = 0.3)
}
for (i in 29:42) {
oldtown_dots <- oldtown_dots %>% addCircleMarkers(data = layers[[i]], group = as.character(region_topic_grid[i,3]), lng = ~lng, lat = ~lat, radius = 4, stroke = TRUE, weight = 1, opacity = 0.3, color = "black", fill = TRUE, fillColor = "blue", fillOpacity = 0.3)
}
for (i in 43:56) {
oldtown_dots <- oldtown_dots %>% addCircleMarkers(data = layers[[i]], group = as.character(region_topic_grid[i,3]), lng = ~lng, lat = ~lat, radius = 4, stroke = TRUE, weight = 1, opacity = 0.3, color = "black", fill = TRUE, fillColor = "green", fillOpacity = 0.3)
}
oldtown_dots <- oldtown_dots %>%
# Add control panel to control base groups (toggle) and overlaygroups (overlaying)
addLayersControl(overlayGroups = region_topic_grid[1:56,3],
baseGroups = c("CartoDB","Esri"),
position = "topright") %>%
# Start with hidden dots
hideGroup(region_topic_grid[1:56,3]) %>%
addFullscreenControl()
oldtown_dots
